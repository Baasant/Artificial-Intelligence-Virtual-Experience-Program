# -*- coding: utf-8 -*-
"""modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RaXpHZ3YvnzH93Omv2ZFHsab2Raqh5dk

# Task 3 - Modeling

This notebook will get you started by helping you to load the data, but then it'll be up to you to complete the task! If you need help, refer to the `modeling_walkthrough.ipynb` notebook.


## Section 1 - Setup

First, we need to mount this notebook to our Google Drive folder, in order to access the CSV data file. If you haven't already, watch this video https://www.youtube.com/watch?v=woHxvbBLarQ to help you mount your Google Drive folder.
"""

from google.colab import drive
drive.mount('/content/drive')

"""We want to use dataframes once again to store and manipulate the data."""

import pandas as pd

"""---

## Section 2 - Data loading

Similar to before, let's load our data from Google Drive for the 3 datasets provided. Be sure to upload the datasets into Google Drive, so that you can access them here.
"""

path = "/content/drive/MyDrive/theforage/cognizant_project/task3/"

sales_df = pd.read_csv(f"{path}sales.csv")
sales_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
sales_df.head()
#sales_df.info()

stock_df = pd.read_csv(f"{path}sensor_stock_levels.csv")
stock_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
stock_df.head()
#stock_df.info()

temp_df = pd.read_csv(f"{path}sensor_storage_temperature.csv")
temp_df.drop(columns=["Unnamed: 0"], inplace=True, errors='ignore')
temp_df.head()

"""Now it's up to you, refer back to the steps in your strategic plan to complete this task. Good luck!"""

sales_df.info()

stock_df.info()

temp_df.info()

#no null value

print(temp_df.shape)
print(sales_df.shape)
print(stock_df.shape)

#we need to merge all 3 dataset into one dataframe 
#we will merge 3 dataset using timestep column

#convert to timestep 
def convert_to_datetime(data: pd.DataFrame = None, column: str = None):

  dummy = data.copy()
  dummy[column] = pd.to_datetime(dummy[column], format='%Y-%m-%d %H:%M:%S')
  return dummy

temp_df=convert_to_datetime(temp_df,'timestamp')
sales_df=convert_to_datetime(sales_df,'timestamp')
stock_df=convert_to_datetime(stock_df,'timestamp')

temp_df.info()

#merge will be hourly 
#so we need to convert timestamp 
from datetime import datetime

def convert_timestamp_to_hourly(data: pd.DataFrame = None, column: str = None):
  dummy = data.copy()
  new_ts = dummy[column].tolist()
  new_ts = [i.strftime('%Y-%m-%d %H:00:00') for i in new_ts]
  new_ts = [datetime.strptime(i, '%Y-%m-%d %H:00:00') for i in new_ts]
  dummy[column] = new_ts
  return dummy

sales_df = convert_timestamp_to_hourly(sales_df, 'timestamp')
sales_df.head()

temp_df = convert_timestamp_to_hourly(temp_df, 'timestamp')
temp_df.head()

stock_df= convert_timestamp_to_hourly(stock_df, 'timestamp')
stock_df.head()

sales_agg = sales_df.groupby(['timestamp', 'product_id']).agg({'quantity': 'sum'}).reset_index()
sales_agg.head()

stock_agg = stock_df.groupby(['timestamp', 'product_id']).agg({'estimated_stock_pct': 'mean'}).reset_index()
stock_agg.head()

temp_agg = temp_df.groupby(['timestamp']).agg({'temperature': 'mean'}).reset_index()
temp_agg.head()

merged_df = stock_agg.merge(sales_agg, on=['timestamp', 'product_id'], how='left')
merged_df.head()

merged_df = merged_df.merge(temp_agg, on='timestamp', how='left')
merged_df.head()

merged_df.shape

# Check columns list and missing values
merged_df.isnull().sum()

#full quantity non values with zero 
merged_df['quantity'] = merged_df['quantity'].fillna(0)
merged_df.info()

#get more features
product_categories = sales_df[['product_id', 'category']]
product_categories = product_categories.drop_duplicates()

product_price = sales_df[['product_id', 'unit_price']]
product_price = product_price.drop_duplicates()

merged_df = merged_df.merge(product_categories, on="product_id", how="left")
merged_df.head()

merged_df = merged_df.merge(product_price, on="product_id", how="left")
merged_df.head()

merged_df=merged_df.drop(['product_id'], axis=1)

merged_df.head()

merged_df.shape



"""#Feature Engineering """

#convert data to numerical 
merged_df['timestamp_day_of_month'] = merged_df['timestamp'].dt.day
merged_df['timestamp_day_of_week'] = merged_df['timestamp'].dt.dayofweek
merged_df['timestamp_hour'] = merged_df['timestamp'].dt.hour
merged_df.drop(columns=['timestamp'], inplace=True)
merged_df.head()

#convert categorical data to numerical 
merged_df = pd.get_dummies(merged_df, columns=['category'])
merged_df.head()

merged_df.shape

X = merged_df.drop(columns=['estimated_stock_pct'])
y = merged_df['estimated_stock_pct']
print(X.shape)
print(y.shape)

#reain the model 
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler

accuracy = []
K=5 #divide data into 10 folder 
split = 0.9
for fold in range(0, K):

  # Instantiate algorithm
  model = RandomForestRegressor()
  scaler = StandardScaler()

  # Create training and test samples
  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)

  # Scale X data, we scale the data because it helps the algorithm to converge
  # and helps the algorithm to not be greedy with large values
  scaler.fit(X_train)
  X_train = scaler.transform(X_train)
  X_test = scaler.transform(X_test)

  # Train model
  trained_model = model.fit(X_train, y_train)

  # Generate predictions on test sample
  y_pred = trained_model.predict(X_test)

  # Compute accuracy, using mean absolute error
  mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)
  accuracy.append(mae)
  print(f"Fold {fold + 1}: MAE = {mae:.3f}")

print(f"Average MAE: {(sum(accuracy) / len(accuracy)):.2f}")